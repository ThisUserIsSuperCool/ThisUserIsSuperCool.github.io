<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>翻译A Recipe for Training Neural Networks | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="翻译A Recipe for Training Neural Networks本文为对A Recipe for Training Neural Networks的翻译（非直译，有删减）。来自：http:&#x2F;&#x2F;karpathy.github.io&#x2F;2019&#x2F;04&#x2F;25&#x2F;recipe&#x2F;其它：37 Reasons why your Neural Network is not working及其网友翻译版本">
<meta property="og:type" content="article">
<meta property="og:title" content="翻译A Recipe for Training Neural Networks">
<meta property="og:url" content="http://example.com/2022/04/02/%E7%BF%BB%E8%AF%91A%20Recipe%20for%20Training%20Neural%20Networks/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="翻译A Recipe for Training Neural Networks本文为对A Recipe for Training Neural Networks的翻译（非直译，有删减）。来自：http:&#x2F;&#x2F;karpathy.github.io&#x2F;2019&#x2F;04&#x2F;25&#x2F;recipe&#x2F;其它：37 Reasons why your Neural Network is not working及其网友翻译版本">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-04-02T07:52:14.000Z">
<meta property="article:modified_time" content="2022-04-02T07:57:57.407Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="translation">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-翻译A Recipe for Training Neural Networks" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/02/%E7%BF%BB%E8%AF%91A%20Recipe%20for%20Training%20Neural%20Networks/" class="article-date">
  <time class="dt-published" datetime="2022-04-02T07:52:14.000Z" itemprop="datePublished">2022-04-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      翻译A Recipe for Training Neural Networks
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="翻译A-Recipe-for-Training-Neural-Networks"><a href="#翻译A-Recipe-for-Training-Neural-Networks" class="headerlink" title="翻译A Recipe for Training Neural Networks"></a>翻译A Recipe for Training Neural Networks</h1><p>本文为对A Recipe for Training Neural Networks的翻译（非直译，有删减）。<br>来自：<a target="_blank" rel="noopener" href="http://karpathy.github.io/2019/04/25/recipe/">http://karpathy.github.io/2019/04/25/recipe/</a><br>其它：<br><a target="_blank" rel="noopener" href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607">37 Reasons why your Neural Network is not working</a>及其<a target="_blank" rel="noopener" href="https://blog.csdn.net/hustqb/article/details/78648556">网友翻译版本</a></p>
<hr>
<p>神经网络是一个leaky abstraction。大量的API，abstraction似乎使我们不必去了解底层原理就能搭建出神经网络，但如果我们真的这样做的话，神经网络很容易就悄摸的产生许多错误。<br>本文是减少神经网课不work带来的痛苦的小棉袄。</p>
<blockquote>
<p>The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.</p>
</blockquote>
<hr>
<h4 id="1-与数据合二为一"><a href="#1-与数据合二为一" class="headerlink" title="1. 与数据合二为一"></a>1. 与数据合二为一</h4><p>训练神经网络前，花时间看看数据。</p>
<ol>
<li>关注数据的分布和异常值。</li>
<li>关注自己分类数据的方式。</li>
</ol>
<p>方式：编写一些简单的代码，对数据进行过滤、搜索、排序。可视化数据可以直观地发现规律和问题。</p>
<h4 id="2-建立简单的模型"><a href="#2-建立简单的模型" class="headerlink" title="2. 建立简单的模型"></a>2. 建立简单的模型</h4><p>做一个简单的小实验。用一个你认为绝对不会搞砸的简单模型建立跑通整个流程：完整的训练+完整的评估。  </p>
<p>这一步的tips：</p>
<ol>
<li>固定random seed。使实验可复现，减少随机性。</li>
<li>简单。减少不必要的花里胡哨。比如data augmentation。</li>
<li>在评估中使用重要的数据。评价时画出基于整个数据集的loss，而不是只基于batch（然后在tensorboard作平滑处理）。</li>
<li>验证以确保正确的loss初始值。</li>
<li>验证以确保正确的最后一层权重初始值。<blockquote>
<p>E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.<br><a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/408811/setting-bias-of-output-layer-for-imbalanced-datasets">https://stats.stackexchange.com/questions/408811/setting-bias-of-output-layer-for-imbalanced-datasets</a></p>
</blockquote>
</li>
<li>把你自己当作baseline。评估人可以理解的指标。比如：准确率呀，预测出来的标签呀。</li>
<li>训练一个input-free模型。通常是训练时把训练数据都设置为0，看模型的输出值，通常这个评估出来的性能会比有数据输入的效果差，以此查看模型是否成功从数据中提取了有用信息。</li>
<li>过拟合一个batch。一直训练一个batch（样本数量可低至2个）直至过拟合，验证loss是可以达到0的。可以在图上可视化，直观观察真实标签和预测标签是否重合，如果没有，不能进行到下一步。</li>
<li>验证减小的training loss。扩大数据集，看你的loss有没有减小。</li>
<li>在数据进入模型前可视化数据。看看数据和标签呗。</li>
<li>动态观测预测过程。我喜欢聚焦test数据的一个batch，观测预测如何改变，有时感觉模型挣扎着要拟合你的数据，比如说有抖动。<blockquote>
<p> Very low or very high learning rates are also easily noticeable in the amount of jitter.</p>
</blockquote>
</li>
<li>使用反向传播来绘制依赖关系。<blockquote>
<p>Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use view instead of transpose&#x2F;permute somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss to be something trivial like the sum of all outputs of example i, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the i-th input. The same strategy can be used to e.g. ensure that your autoregressive model at time t only depends on 1..t-1. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.</p>
</blockquote>
</li>
<li>先写specific function，再把它改成generalized版本。<br>####3.过拟合<br>在这一步，我们已经对数据有了充分了解，并有了一个可以正常运行的小模型和评估架子了。这样我们可以计算得到任何我们可以信任的指标了。下一步可以训练出一个好的模型了。<br>为了找到一个好的模型，下一步是：（1）找到一个模型使它过拟合（关注training loss），（2）并且进行正则化（牺牲部分training loss以提升validation loss）。试想一下，如果无法找到任何一个模型达到一个低错误率，这说明之前有些问题。</li>
</ol>
<p>过拟合这一步的tips：</p>
<ul>
<li><p>挑选模型。尤其在早期，不要搞些花里胡哨的。我总是建议人们找到最相关的文章直接复制他们最简单的结构，以获得良好的性能。</p>
<ul>
<li>adam是最安全的。训练早期强推Adam with a learning rate of [3e-4]。Adam对超参数很能忍，即使学习率真的很烂。例如，对ConvNets而言，一个精心调整的SGD总是比Adam的效果要好一些，但理想学习率区间却要窄，而且是针对特定问题的。（注意：If you are using RNNs and related sequence models it is more common to use Adam.）</li>
<li>一次只把一个地方搞复杂。每次一个plug-in，并确保有你预期的性能提升。</li>
<li>别相信默认的learning rate decay。注意学习率的衰减，尤其当从其它领域搞过来代码时。如果你不是太小心的话，你的学习率会由于不好的衰减率，可能太早就衰减到0。通常，I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.</li>
</ul>
<blockquote>
<p>Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset.</p>
</blockquote>
</li>
</ul>
<h4 id="4-正则化"><a href="#4-正则化" class="headerlink" title="4.正则化"></a>4.正则化</h4><p>理想情况下，我们已经拥有了一个至少适配数据集的大模型了。是时候进行正则化，并通过牺牲一些training acc以换取validation acc了。</p>
<p>正则化这一步的tips：</p>
<ul>
<li>更多数据。一个配置良好的网络需要更多数据。另一个提升性能的方法是ensembles（如果你可以承受），但最多5个模型就到顶了。</li>
</ul>
<ul>
<li><p>data augment。积极尝试半假数据。</p>
</li>
<li><p>creative augmentation。如果上面那种半假数据没用的话，不如直接尝试假数据。</p>
<blockquote>
<p>For example, <a target="_blank" rel="noopener" href="https://openai.com/blog/learning-dexterity/">domain randomization</a>, use of <a target="_blank" rel="noopener" href="http://vladlen.info/publications/playing-data-ground-truth-computer-games/">simulation</a>, clever <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.01642">hybrids</a> such as inserting (potentially simulated) data into scenes, or even GANs.</p>
</blockquote>
</li>
</ul>
<ul>
<li>预训练模型：即使你有足够多的数据，使用预训练模型也会丝滑很多。</li>
</ul>
<ul>
<li><p>我劝你坚持监督学习。目前没有在机器视觉这一块儿看到无监督训练特别牛逼。</p>
</li>
<li><p>减小输入的维度。删除那些含有杂散信息的特征。如果你的数据集特别小，这些spurious signal只会增加你过拟合的机会。</p>
</li>
<li><p>减小模型大小。很多情况下，可以用领域知识减小模型的大小。</p>
<blockquote>
<p>As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.</p>
</blockquote>
</li>
<li><p>减小batch大小。由于batch norm，较小的batch大小在某种程度上对应于更强的正则化。</p>
<blockquote>
<p>Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean&#x2F;std are more approximate versions of the full mean&#x2F;std so the scale &amp; offset “wiggles” your batch around more.</p>
</blockquote>
</li>
<li><p>加入dropout。小心使用，因为<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.05134">dropout和batch norm好像不太搭</a>。</p>
</li>
<li><p>权重衰减。增加权重衰减的惩罚。</p>
</li>
<li><p>及时停止：基于你的验证数据，在过拟合前停止。</p>
</li>
<li><p>尝试大一点的模型：性能更佳哟。</p>
</li>
</ul>
<p>最后，为了确认这个模型是一个合理的分类器，我喜欢可视化网络的第一层的权重。确保这些权重有良好的边缘，如果它们看起来像是噪点，那么可能出现问题了。同样的，activation也能反映问题哟。</p>
<h4 id="5-调参"><a href="#5-调参" class="headerlink" title="5.调参"></a>5.调参</h4><p>调参的tips：</p>
<ul>
<li>随机搜索而不是网格搜索。使用grid search一次性调整多个参数的注意很不错，但记得坚持随机搜索。这是因为神经网络对一些参数远比另一些更敏感，不如对更重要的参数多采一些样吧。</li>
<li>超参数优化。使用花哨的bayesian hyper-parameter optimization toolboxes吧，或者更好地是使用实习生（狗头）。</li>
</ul>
<h4 id="6-压榨最后一滴性能"><a href="#6-压榨最后一滴性能" class="headerlink" title="6.压榨最后一滴性能"></a>6.压榨最后一滴性能</h4><p>除了最好的结构和最好的超参数，你还可以：</p>
<ul>
<li>ensembles：模型集成可以保证2%的性能提升。如果你的算力到达了极限，可以尝试 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.02531">dark knowledge</a>来将你的ensemble处理成一个网咯。</li>
<li>让它跑着吧。说不定跑久了就是sota了。</li>
</ul>
<p>读到这儿，你已经知道了成功所需的所有ingredients了，没错，我说的是：对技术的深刻理解，数据集和提出的问题，你设置了一整个训练&#x2F;评估结构，并对它计算出的acc有十足的信心，接着你开发了更加复杂的模型，在你改动的每一步都有性能的提升。我想，你已经准备大量打量文章，做大量实验，并得到你的sota结果了。</p>
<blockquote>
<p>Good Luck!</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/02/%E7%BF%BB%E8%AF%91A%20Recipe%20for%20Training%20Neural%20Networks/" data-id="cl1hkcoat00008hp06gqncxx5" data-title="翻译A Recipe for Training Neural Networks" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/translation/" rel="tag">translation</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2022/04/02/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/translation/" rel="tag">translation</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/translation/" style="font-size: 10px;">translation</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/02/%E7%BF%BB%E8%AF%91A%20Recipe%20for%20Training%20Neural%20Networks/">翻译A Recipe for Training Neural Networks</a>
          </li>
        
          <li>
            <a href="/2022/04/02/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>